{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import click\n",
    "import datetime\n",
    "import math\n",
    "import typing\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import types as T, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from spark_lib import ss_udf, ss_const, spark_utils, ss_common\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zipindex_to_column(row_idx):\n",
    "    row, idx = row_idx\n",
    "    dic = row.asDict()\n",
    "    dic['domain_id'] = idx\n",
    "    return T.Row(**dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main部分\n",
    "def main(date):\n",
    "    \"\"\"\n",
    "    main部分動かしたら全部いい感じになるように実装\n",
    "    \"\"\"\n",
    "    date = str(date)\n",
    "    date_obj = datetime.datetime.strptime(date, '%Y%m%d')\n",
    "    date_from = (date_obj - datetime.timedelta(days=10)).strftime('%Y%m%d')\n",
    "    \n",
    "    \n",
    "    # ここの部分はdata_getに置き換えて取れるようにする\n",
    "    posi_df = spark.read.parquet(\"posi_df\")\n",
    "    nega_df = spark.read.parquet(\"df_nega\")\n",
    "\n",
    "    # domain_idsをgetする\n",
    "    domain_ids = get_domain_ids(date_from, date)\n",
    "    \n",
    "    bc_domain_ids = spark.sparkContext.broadcast(\n",
    "        domain_ids.toPandas().set_index(\"domain\").domain_id.to_dict()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #特徴の最大数を獲得\n",
    "    feature_count = spark.table('mining.domain_id_mappings') \\\n",
    "    .where(F.col('dt') == date) \\\n",
    "    .select(F.max('domain_id').alias('max_id')) \\\n",
    "    .collect()[0].max_id + 1\n",
    "    \n",
    "    # モデル作成, 評価\n",
    "    for ssp_page_id, page_ids in ss_const.TARGET_SSP_PAGE_ID.items():\n",
    "        posi_feature_df,  nega_feature_df = feature_create(date_from, date, ssp_page_id, page_ids, bc_domain_ids)\n",
    "        ### ここ修正必要あり(posi_feature, nega_feature→df) ###\n",
    "        posi_feature_df.withColumn(\"\")\n",
    "        for i in [\"cova\", \"gbdt\"]:\n",
    "            model, train_df, test_df = model_kind_choice(df, model_kind = str(i))\n",
    "            model_evaluation(model, train_df, test_df,thresholds=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_get(advertisers_id=None, order_id = None):\n",
    "#     \"\"\"\n",
    "#     data取得part\n",
    "#     \"\"\"\n",
    "#     spark.table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_ids(date_from, date):\n",
    "    df = (\n",
    "        spark.table(\"mining.uuid_weekly_domains\")\n",
    "        .where(\n",
    "            F.col(\"dt\").between(date_from, date)\n",
    "            & F.col(\"uuid\").isin(*ss_const.EXCLUDE_UUID)\n",
    "            == False\n",
    "        )\n",
    "        .selectExpr(\"uuid\", \"map_keys(domains) AS domains\")\n",
    "        .groupBy(\"uuid\")\n",
    "        .agg(F.array_distinct(F.flatten(F.collect_list(\"domains\"))).alias(\"domains\"))\n",
    "    )\n",
    "\n",
    "    domain_ids = (\n",
    "        df.select(F.explode(\"domains\").alias(\"domain\"))\n",
    "        .groupby(\"domain\")\n",
    "        .agg(F.count(F.lit(1)).alias(\"domain_count\"))\n",
    "        .where(F.col(\"domain_count\") >= 500)\n",
    "        .rdd.zipWithIndex()\n",
    "        .map(_zipindex_to_column)\n",
    "        .toDF()\n",
    "        .select(\"domain\", \"domain_id\")\n",
    "        .repartition(1000)\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    return domain_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_create(\n",
    "    date_from: str,\n",
    "    date_to: str,\n",
    "    ssp_page_id: int,\n",
    "    page_ids: typing.List[int],\n",
    "    bc_domain_ids,\n",
    "):\n",
    "    \"\"\"\n",
    "    特徴量作成パート1\n",
    "    \"\"\"\n",
    "\n",
    "    df = (\n",
    "        spark.table(\"mining.uuid_weekly_domains\")\n",
    "        .where(\n",
    "            F.col(\"dt\").between(date_from, date_to)\n",
    "            & F.col(\"ssp_page_id\").isin(page_ids)\n",
    "            & (F.col(\"uuid\").isin(*ss_const.EXCLUDE_UUID) == False)\n",
    "        )\n",
    "        .select(\"uuid\", \"domains\", \"device_type\")\n",
    "    )\n",
    "\n",
    "    rdd = df.rdd.mapPartitions(lambda rows: bc_map_join(rows, bc_domain_ids))\n",
    "    # .toDF(['uuid', 'feature_ids'])\n",
    "    schema = T.StructType(\n",
    "        [\n",
    "            T.StructField(\"uuid\", T.StringType(), True),\n",
    "            T.StructField(\"feature_ids\", T.ArrayType(T.IntegerType()), True),\n",
    "            T.StructField(\"device_type\", T.StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    df = (\n",
    "        spark.createDataFrame(rdd, schema)\n",
    "        .groupBy(\"uuid\")\n",
    "        .agg(\n",
    "            F.when(F.array_contains(F.collect_set(\"device_type\"), \"app\"), 1)\n",
    "            .otherwise(0)\n",
    "            .alias(\"has_app\"),\n",
    "            F.array_sort(\n",
    "                F.array_distinct(F.flatten(F.collect_list(\"feature_ids\")))\n",
    "            ).alias(\"feature_ids\"),\n",
    "        )\n",
    "        .where(F.size(\"feature_ids\") >= 5)\n",
    "        .persist()\n",
    "    )\n",
    "    \n",
    "    posi_feature_df = df.join(posi_df)\n",
    "    nega_feature_df = df.join(nega_df)\n",
    "    \n",
    "    return posi_feature_df, nega_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_kind_choice(df, model_kind = \"cova\"):\n",
    "    if model_kind == \"cova\":\n",
    "        train_df, test_df = df.randomSplit([0.9, 0.1])\n",
    "        lr = LogisticRegression(maxIter=100, regParam=0.05, elasticNetParam=0.1)\n",
    "        model = lr.fit(train_df)\n",
    "        \n",
    "    elif model_kind == \"gbdt\":\n",
    "        train_df, test_df = df.randomSplit([0.9, 0.1])\n",
    "        labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "        featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "        gbt = GBTClassifier(labelcol = \"indexedLabel\", featurescol = \"indexedFeatures\")\n",
    "        pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "        model = pipeline.fit(train_df)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"error : input is cova or lgbt only\")\n",
    "    \n",
    "    return model, test_df, train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, train_df, test_df,thresholds=[0.5]):\n",
    "    \"\"\"\n",
    "    モデルを評価するためのpart\n",
    "    \"\"\"\n",
    "    prediction = model.transform(test_df)\n",
    "\n",
    "    # metrics\n",
    "    second_element = F.udf(lambda v: float(v[1]), T.DoubleType())\n",
    "    prediction = prediction.withColumn('probability', second_element('probability')) \\\n",
    "        .withColumn('label', prediction.label.cast(T.DoubleType()))\n",
    "    metrics = BinaryClassificationMetrics(prediction.select('probability', 'label').rdd)\n",
    "    print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "    print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n",
    "\n",
    "    # pr/rec\n",
    "    cnt = prediction.count()\n",
    "    for t in thresholds:\n",
    "        record = {}\n",
    "        record['threshold'] = t\n",
    "        record['pr_auc'] = metrics.areaUnderPR\n",
    "        record['roc_auc'] = metrics.areaUnderROC\n",
    "        tp = prediction.where((F.col('label') == 1.0) & (F.col('probability') >= t)).count()\n",
    "        fp = prediction.where((F.col('label') != 1.0) & (F.col('probability') >= t)).count()\n",
    "        tn = prediction.where((F.col('label') != 1.0) & (F.col('probability') < t)).count()\n",
    "        fn = prediction.where((F.col('label') == 1.0) & (F.col('probability') < t)).count()\n",
    "        if tp + fp == 0:\n",
    "            continue\n",
    "        lift = (tp / (tp + fp)) / ((tp + fp) / (tp + fp + tn + fn))\n",
    "        record['tp'] = tp\n",
    "        record['fp'] = fp\n",
    "        record['tn'] = tn\n",
    "        record['fn'] = fn\n",
    "        record['lift'] = lift\n",
    "        if tp + fp == 0:\n",
    "            record['pr'] = 0.0\n",
    "            record['rec'] = 0.0\n",
    "        else:\n",
    "            record['pr'] = tp / (tp + fp)\n",
    "            record['rec'] = tp / (tp + fn)\n",
    "        result.append(record)\n",
    "        print(record)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(20200501)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
